{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Task 2: Regularizations\n",
    "\n",
    "In this task, you are going to experiment with two popular regularization methods. \n",
    "\n",
    "**Batch normalization:** Batch normalization makes it possible to train a deep network. When a network becomes deeper, the distribution of hidden neurons' values will also shift greatly, and that is one reason that makes it difficult to train a deep neural network. Machine learning taught us that normalization is a good preprocessing method to deal with such a problem. Therefore, batch normalization deploys a similar idea in neural networks by re-normalizing the hidden values of each layer before transfering values to the next layer.\n",
    "\n",
    "**Dropout:** In the last assignment, you trained a shallow network and everything looked fine. However, when the network becomes larger and deeper, it will also become harder to train. The first potential problem is overfitting, that is, the network overreacts to noise or random errors of the training data while failing to detect the underlying distribution pattern. It is more likely to occur when the model becomes complex and contains more trainable parameters. Dropout is a well-known method that can eliminate such effects. The core idea behind it is quite simple: rather than updating all trainable parameters each time, it randomly selects a subset of parameters to update and keeps other parameters unaltered.\n",
    "\n",
    "* References\n",
    "    * https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf\n",
    "    * https://arxiv.org/pdf/1502.03167.pdf\n",
    "    * https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization\n",
    "    * https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw Fashion-MNIST data.\n",
    "train, val = fashion_mnist.load_data()\n",
    "\n",
    "X_train_raw, y_train = train\n",
    "X_val_raw, y_val = val\n",
    "\n",
    "X_train = X_train_raw.reshape((X_train_raw.shape[0], X_train_raw.shape[1]**2))\n",
    "X_val = X_val_raw.reshape((X_val_raw.shape[0], X_val_raw.shape[1]**2))\n",
    "\n",
    "#Index from the 10000th image of the dataset\n",
    "# X_val = X_train[10000:10500,:]\n",
    "# y_val = y_train[10000:10500]\n",
    "# X_train = X_train[10500:12500,:]\n",
    "# y_train = y_train[10500:12500]\n",
    "\n",
    "mean_image = np.mean(X_train, axis=0).astype(np.float32)\n",
    "X_train = X_train.astype(np.float32) - mean_image\n",
    "X_val = X_val.astype(np.float32) - mean_image\n",
    "\n",
    "# We've vectorized the data for you. That is, we flatten the 32×32×3 images into 1×3072 Numpy arrays.\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement batch normalization forward function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some batched input data $X_b = (x_1, \\dots, x_N)^T \\in R^{N \\times D}$, $x_i \\in R^D$, $N$ is the batch size, $b = 1 \\dots B$ is the batch index (e.g. $X_1$ denotes the 1st batch), $B$ is the total number of batches and $D$ is the dimension of the data. \n",
    "\n",
    "To do a batch normalization, first we estimat the normalized value of the inputs\n",
    "\n",
    "$$\n",
    "\\hat{X}_b = \\frac{X_b - \\mu_b}{\\sqrt{\\sigma_b^2 + \\epsilon}} \\in R^{N \\times D} \\quad \n",
    "\\text{where} \\quad\n",
    "\\begin{cases}\n",
    "\\mu_b = \\alpha \\mu_{b-1} + (1 - \\alpha) \\bar{X}_b \\\\\n",
    "\\sigma_b^2 = \\alpha \\sigma_{b-1} + (1 - \\alpha) S^2(X_b) \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "\\bar{X}_b = \\frac{1}{N} \\sum_{i=1}^N x_i \\in R^D, \\quad\n",
    "S^2(X_b) = \\frac{1}{N} \\sum_{i=1}^N (x_i - \\bar{X}_b)^2 \\in R^D\n",
    "$$\n",
    "\n",
    "Here, $\\alpha$ is the decay factor, $\\mu_b$ and $\\sigma_b^2$ are the ***moving mean*** and ***moving variance*** for batch $b$. They are defined by an exponentially decaying sum of the mean / variance from all previous batches (from $1$ to $b - 1$) and the actual sample mean / variance of the current batch. \n",
    "\n",
    "Both $\\mu_0$ and $\\sigma_0^2$ are initialized to be $\\mathbb{0}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that considering the simplicity of expressions, we are actually abusing the shapes of the variables. For the term $X_b - \\mu_b$, in reality we have $X_b \\in R^{N \\times D}$ and $\\mu_b \\in R^D$. As a result, when doing $X_b - \\mu_b$ we are actually subtracting $\\mu_b$ from every one of the N rows from $X_b$ (i.e. the formula should be $X_b - \\mathbb{1} \\mu_b^T$), likewise for the $\\sigma_b^2 \\in R^D$ in the denominator. Of course, when you're writing codes, the [broadcasting mechanism](https://numpy.org/doc/stable/user/basics.broadcasting.html) in numpy will automatically do this for you. See the details in the **./utils/reg_funcs.py**. \n",
    "\n",
    "Also note that $S^2$ here denotes the **variance**, however in the field machine learning (and statistics) it more commonly stands for **sample variance** which is something that's slightly different. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the batch normalization is computed as\n",
    "\n",
    "$$\n",
    "Y_b = \\hat{X}_b \\odot \\gamma + \\beta\n",
    "$$\n",
    "\n",
    "where $\\beta, \\gamma \\in R^D$ are **learnable parameters** corresponding to the normalized mean and standard deviation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO:</strong></span> Edit functions `bn_forward` in **./utils/reg_funcs.py**\n",
    "\n",
    "If the code is running correctly, **mean of a2_bn for train will be very close to 0 and variance of a2_bn will be close to 1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# Checking/verification code. Don't change it.     #\n",
    "####################################################\n",
    "\n",
    "from utils.reg_funcs import bn_forward\n",
    "from utils.reg_funcs import bn_backward\n",
    "\n",
    "np.random.seed(2022)\n",
    "N, D, H1, H2 = 200, 64, 3, 3\n",
    "eps = 1e-5\n",
    "x_in = np.random.randn(N, D)\n",
    "w1 = np.random.randn(D,H1)\n",
    "w2 = np.random.randn(H1,H2)\n",
    "a2 = np.maximum(x_in.dot(w1),0).dot(w2)\n",
    "\n",
    "# Before batch normalization\n",
    "print(\"mean of a2: \", np.mean(a2, axis=0))\n",
    "print(\"var of a2: \", np.var(a2, axis=0))\n",
    "\n",
    "# Test \"train mode\" of forward function\n",
    "# After batch normalization, the mean should be close to zero and var should be close to one. \n",
    "bn_config = {\"epsilon\":eps, \"decay\":0.9}\n",
    "gamma = np.ones(H2)\n",
    "beta = np.zeros(H2)\n",
    "a2_bn, _ = bn_forward(a2, gamma, beta, bn_config, \"train\")\n",
    "print(\"(train) mean of a2_bn:\", np.mean(a2_bn, axis=0))\n",
    "print(\"(train) var of a2_bn:\", np.var(a2_bn, axis=0))\n",
    "print('*'*80)\n",
    "print(\"Is mean correct? -\", np.allclose(np.mean(a2_bn, axis=0), np.zeros_like(np.mean(a2_bn, axis=0))))\n",
    "print(\"Is variance correct? -\", np.allclose(np.var(a2_bn, axis=0), np.ones_like(np.var(a2_bn, axis=0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO:</strong></span> Test \"moving average\" and \"test mode\" of the `bn_forward` function. \n",
    "\n",
    "We expect that the test mean & variance of a2 to be fairly close to the real values and the test mean & variance of a2_bn to be close to 0 and 1 respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "# Checking code. Don't change it.     #\n",
    "#######################################\n",
    "\n",
    "# Test \"moving average\" and \"test mode\" of forward function\n",
    "# Then you are going to run the forward function under \"training mode\" for several times, \n",
    "# and the moving mean and moving var will be close to the real mean and var of the input data.\n",
    "# Next, run the forward function under \"test\" mode and you will see that the mean and var of its \n",
    "# output will be also close to gamma, beta that you have set before.\n",
    "\n",
    "bn_config = {\"epsilon\":1e-8, \"decay\":0.8}\n",
    "gamma = np.ones(H2)\n",
    "beta = np.zeros(H2)\n",
    "\n",
    "# collect_data: for calculating real mean and var of a2 later.\n",
    "collect_data = a2\n",
    "np.random.seed(2022)\n",
    "for _ in range(100):\n",
    "    x_in = np.random.randn(N, D)\n",
    "    a2 = np.maximum(x_in.dot(w1),0).dot(w2)\n",
    "    collect_data = np.concatenate((collect_data, a2), axis=0)\n",
    "    bn_forward(a2, gamma, beta, bn_config, \"train\")\n",
    "\n",
    "# compare moving_mean and moving_var with real mean and var.\n",
    "# You should see that they are close to each other.\n",
    "print(\"real mean of data: \", np.mean(collect_data, axis=0))\n",
    "print(\"real var of data: \", np.var(collect_data, axis=0))\n",
    "print(\"moving mean of data: \", bn_config[\"moving_mean\"])\n",
    "print(\"moving var of data: \", bn_config[\"moving_var\"])\n",
    "\n",
    "# \"test mode\" of forward function\n",
    "# After bn_forward, the mean and var of output should be kind of close to gamma and beta.\n",
    "x_in = np.random.randn(N, D)\n",
    "a2 = np.maximum(x_in.dot(w1),0).dot(w2)\n",
    "print(\"*\"*80)\n",
    "print(\"(test) mean of a2: \", np.mean(a2, axis=0))\n",
    "print(\"(test) var of a2: \", np.var(a2, axis=0))\n",
    "a2_bn, _ = bn_forward(a2, gamma, beta, bn_config, \"test\")\n",
    "print(\"(test) mean of a2_bn: \", np.mean(a2_bn, axis=0))\n",
    "print(\"(test) var of a2_bn: \", np.var(a2_bn, axis=0))\n",
    "print(\"*\"*80)\n",
    "print(\"Is moving mean correct? -\", np.allclose(bn_config[\"moving_mean\"], np.mean(collect_data, axis=0), rtol=1e-1))\n",
    "print(\"Is moving variance correct? -\", np.allclose(bn_config[\"moving_var\"], np.var(collect_data, axis=0), rtol=1e-1))\n",
    "print(\"Is test mean correct? -\", np.allclose(np.mean(a2_bn, axis=0), beta, atol=2e-1))\n",
    "print(\"Is test variance correct? -\", np.allclose(np.var(a2_bn, axis=0), gamma, atol=2e-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch normalization backward function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do a back propogation, just simply take the derivitives. Assume that the upstream gradient from the final output $Z$ w.r.t the layer output $Y$ is $\\nabla_Y Z = g \\in R^{N \\times D}$, the backward pass is\n",
    "\n",
    "$$\n",
    "\\nabla_{X} Z = \\frac{1}{\\sqrt{\\sigma^2+\\epsilon}} \\odot \\gamma \\odot g \\in R^{N \\times D}, \\quad \n",
    "\\nabla_{\\gamma} Z = \\mathbb{1}^T (g \\odot \\frac{X - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}) \\in R^D, \\quad\n",
    "\\nabla_{\\beta} Z = \\mathbb{1}^T g \\in R^D\n",
    "$$\n",
    "\n",
    "Here we use $\\mu = \\mu_B$ and $\\sigma = \\sigma_B$ (the final moving mean / variance we cached after the last batch of input in the forward stage). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO:</strong></span> Use TensorFlow 2 functions to verify the correctness of the backward function. \n",
    "\n",
    "<span style=\"color:red\"><strong>Hint:</strong></span> Use `tf.GradientTape()`. You may find an example usage in the instructor's verification code from Assignment 1 Task1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After verifying the forward function and save the bn_config.\n",
    "x_in = np.random.randn(N, D)\n",
    "a2 = np.maximum(x_in.dot(w1),0).dot(w2)\n",
    "da2_bn = np.ones_like(a2)\n",
    "# Test backward function with tensorflow\n",
    "# You will use bn_config = {\"eps\":1e-5, \"decay\":0.9, \"moving_mean\":moving_mean, \"moving_var\":moving_var}\n",
    "gamma = np.ones(H2)\n",
    "beta = np.zeros(H2)\n",
    "a2_bn, cache = bn_forward(a2, gamma, beta, bn_config, \"test\")\n",
    "da2, dgamma, dbeta = bn_backward(da2_bn, cache)\n",
    "\n",
    "# results\n",
    "da2_tf = tf.zeros_like(da2)\n",
    "dgamma_tf = tf.zeros_like(dgamma)\n",
    "dbeta_tf = tf.zeros_like(dbeta)\n",
    "\n",
    "###################################################\n",
    "# TODO: verify the backward code. You should use  #\n",
    "# the parameters in bn_config and other variables #\n",
    "# above.                                          #\n",
    "# You should store:                               #\n",
    "#     - da2_tf: gradient of a2_bn w.r.t a2        #\n",
    "#     - dgamma_tf: gradient of a2_bn w.r.t gamma  #\n",
    "#     - dbeta_tf: gradient of a2_bn w.r.t beta    #\n",
    "###################################################\n",
    "# raise NotImplementedError\n",
    "\n",
    "\n",
    "###################################################\n",
    "# ENDTODO #\n",
    "###################################################\n",
    "    \n",
    "# Make comparison\n",
    "print(\"Is a2_bn correct? {}\".format(np.allclose(a2_bn, a2_bn_check)))\n",
    "print(\"Is da2 correct? {}\".format(np.allclose(da2, da2_check)))\n",
    "print(\"Is dgamma correct? {}\".format(np.allclose(dgamma, dgamma_check)))\n",
    "print(\"Is dbeta correct? {}\".format(np.allclose(dbeta, dbeta_check)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization Experiments with MLP\n",
    "\n",
    "<span style=\"color:red\"><strong>TODO:</strong></span>\n",
    "\n",
    "1. Add batch normalization into MLP in `./utils/neuralnets/mlp.py`\n",
    "\n",
    "2. First create a shallow MLP like two-layer network with shape [50(+10)] (a hidden layer with depth 5 and an output layer of 10 classes). Train it without and with batch normalization. Plot the loss, training accuracy, and validation accuracy curves.\n",
    "\n",
    "3. Then, create a slightly deeper 3-layer MLP network with shape [50,20(+10)] and train the network with and without batch normalization. Plot the loss, training accuracy, and validation accuracy curves.\n",
    "\n",
    "4. Make a comparison and describe what you have found in this experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment on shallow MLP** \n",
    "\n",
    "Here in the demo, we set a large learning rate on purpose. You can play with different learning rates and see what happens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.neuralnets.mlp import MLP \n",
    "from utils.optimizers import AdamOptim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a two-layer network without batch normalization.\n",
    "# Here is a demo.\n",
    "use_bn = False\n",
    "model = MLP(input_dim=X_train.shape[1], hidden_dims=[50], num_classes=10, \n",
    "            weight_scale=1e-3, l2_reg=0.0, use_bn=use_bn)\n",
    "optimizer = AdamOptim(model)\n",
    "# hist_no_dropout contains loss, train acc and valid acc history.\n",
    "hist_shallow_no_bn = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                           num_epoch=5, batch_size=100, learning_rate=2e-2, learning_decay=0.95, \n",
    "                           verbose=False, record_interval = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a two-layer network with batch normalization. Remember to \"use_bn\".\n",
    "use_bn = True\n",
    "model = MLP(input_dim=X_train.shape[1], hidden_dims=[50], num_classes=10, \n",
    "            weight_scale=1e-3, l2_reg=0.0, use_bn=use_bn)\n",
    "optimizer = AdamOptim(model)\n",
    "# hist_no_dropout contains loss, train acc and valid acc history.\n",
    "hist_shallow_bn = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                           num_epoch=5, batch_size=100, learning_rate=2e-2, learning_decay=0.95, \n",
    "                           verbose=False, record_interval = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bn and no_bn results together in three plots and make a comparison. \n",
    "title_name = [\"loss\", \"train acc\", \"val acc\"]\n",
    "_, axarr = plt.subplots(1,3, figsize=(15,5))\n",
    "for i in range(3):\n",
    "    axarr[i].plot(hist_shallow_no_bn[i], label=\"no_bn\")\n",
    "    axarr[i].plot(hist_shallow_bn[i], label=\"bn\")\n",
    "    axarr[i].legend(), axarr[i].set_title(title_name[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment on deep MLP** \n",
    "\n",
    "Here in the demo, we set a large learning rate on purpose. You can play with different learning rates and see what happens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a deep 5-layer network without batch normalization. Remember to \"use_bn\".\n",
    "use_bn = False\n",
    "model = MLP(input_dim=X_train.shape[1], hidden_dims=[100, 50], num_classes=10, \n",
    "            weight_scale=1e-3, l2_reg=0.0, use_bn=use_bn)\n",
    "optimizer = AdamOptim(model)\n",
    "# hist_no_dropout contains loss, train acc and valid acc history.\n",
    "hist_deep_no_bn = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                           num_epoch=5, batch_size=100, learning_rate=2e-2, learning_decay=0.95, \n",
    "                           verbose=False, record_interval = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a deep 5-layer network with batch normalization. Remember to \"use_bn\".\n",
    "use_bn = True\n",
    "model = MLP(input_dim=X_train.shape[1], hidden_dims=[100, 50], num_classes=10, \n",
    "            weight_scale=1e-3, l2_reg=0.0, use_bn=use_bn)\n",
    "optimizer = AdamOptim(model)\n",
    "# hist_no_dropout contains loss, train acc and valid acc history.\n",
    "hist_deep_bn = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                           num_epoch=5, batch_size=100, learning_rate=2e-2, learning_decay=0.95, \n",
    "                           verbose=False, record_interval = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bn and no_bn results together in three plots and make a comparison. \n",
    "title_name = [\"loss\", \"train acc\", \"val acc\"]\n",
    "_, axarr = plt.subplots(1,3, figsize=(15,5))\n",
    "for i in range(3):\n",
    "    axarr[i].plot(hist_deep_no_bn[i], label=\"no_bn\")\n",
    "    axarr[i].plot(hist_deep_bn[i], label=\"bn\")\n",
    "    axarr[i].legend(), axarr[i].set_title(title_name[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO:</strong></span> Describe what you find in this experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: **[fill in here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement dropout_forward function\n",
    "\n",
    "<span style=\"color:red\"><strong>TODO:</strong></span> Edit function `dropout_forward` in `./utils/reg_funcs.py`. If the code is running correctly, you will see that the outputs of the verification code should be close to each other. \n",
    "\n",
    "If the function is correct, then **the output mean should be close to the input mean. Input mean and output test mean should be identical.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the forward stage of dropout, each entry in the original data are kept with a certain probability $p$, otherwise they are discarded (set to zero). Assume the input $X \\in R^{N \\times D}$, the forward pass is\n",
    "\n",
    "$$Y = M \\odot X \\in R^{N \\times D}$$\n",
    "\n",
    "where $M \\sim B(N \\times D, p)$ is a boolean mask generated from Binomial distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_in = np.random.randn(500, 500) + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "# Checking/verification code. Don't change it. #\n",
    "################################################\n",
    "from utils.reg_funcs import dropout_forward\n",
    "from utils.reg_funcs import dropout_backward\n",
    "\n",
    "p = 0.7\n",
    "dropout_config = {\"enabled\": True, \"keep_prob\": p}\n",
    "# feedforward\n",
    "out, cache = dropout_forward(x=x_in, dropout_config=dropout_config, mode=\"train\")\n",
    "out_test, _ = dropout_forward(x=x_in, dropout_config=dropout_config, mode=\"test\") \n",
    "# backward\n",
    "dout = np.ones_like(x_in)\n",
    "dx = dropout_backward(dout, cache)\n",
    "################################################\n",
    "# Checking/verification code. Don't change it. #\n",
    "################################################\n",
    "# Check forward correctness\n",
    "print(\"mean_of_input = {}\".format(p*np.mean(x_in)))\n",
    "print(\"mean_of_out = {}\".format(np.mean(out)))\n",
    "print(\"mean_of_out_test = {}\".format(np.mean(out_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout Experiments with MLP\n",
    "\n",
    "<span style=\"color:red\"><strong>TODO:</strong></span>\n",
    "\n",
    "1. Add dropout into the MLP in `./utils/neuralnets/mlp.py` and understand how the dropout is added into the MLP.\n",
    "\n",
    "2. Customize your own MLP network. Then, train networks with different $p$ of $\\{0.1, 0.3, 0.5, 0.7, 0.9, 1\\}$. If $p = 1$, then the network is the MLP without dropout. \n",
    "\n",
    "3. Plot the loss, training accuracy, and validation accuracy curves.\n",
    "\n",
    "Note that checking/validation code is included below with preselected dropout parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is an example on how to collect loss and accuracy info\n",
    "dropout_config = {\"enabled\":True, \"keep_prob\": 1}\n",
    "model = MLP(input_dim=X_train.shape[1], hidden_dims=[200], num_classes=10, \n",
    "            weight_scale=1e-3, l2_reg=0.0, dropout_config=dropout_config)\n",
    "optimizer = AdamOptim(model)\n",
    "# hist_no_dropout contains loss, train acc and valid acc history.\n",
    "hist_no_dropout = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                           num_epoch=5, batch_size=100, learning_rate=1e-3, learning_decay=0.95, \n",
    "                           verbose=False, record_interval = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_config[\"keep_prob\"] = 0.9\n",
    "model = MLP(input_dim=X_train.shape[1], hidden_dims=[200], num_classes=10, \n",
    "            weight_scale=1e-2, l2_reg=0.0, dropout_config=dropout_config)\n",
    "optimizer = AdamOptim(model)\n",
    "hist_dropout_9 = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                           num_epoch=5, batch_size=100, learning_rate=1e-3, learning_decay=0.95,\n",
    "                           verbose=False, record_interval = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_config[\"keep_prob\"] = 0.7\n",
    "model = MLP(input_dim=X_train.shape[1], hidden_dims=[200], num_classes=10, \n",
    "            weight_scale=1e-2, l2_reg=0.0, dropout_config=dropout_config)\n",
    "optimizer = AdamOptim(model)\n",
    "hist_dropout_7 = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                           num_epoch=5, batch_size=100, learning_rate=1e-3, learning_decay=0.95,\n",
    "                           verbose=False, record_interval = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_config[\"keep_prob\"] = 0.5\n",
    "model = MLP(input_dim=X_train.shape[1], hidden_dims=[200], num_classes=10, \n",
    "            weight_scale=1e-2, l2_reg=0.0, dropout_config=dropout_config)\n",
    "optimizer = AdamOptim(model)\n",
    "hist_dropout_5 = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                           num_epoch=5, batch_size=100, learning_rate=1e-3, learning_decay=0.95,\n",
    "                           verbose=False, record_interval = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_config[\"keep_prob\"] = 0.3\n",
    "model = MLP(input_dim=X_train.shape[1], hidden_dims=[200], num_classes=10, \n",
    "            weight_scale=1e-2, l2_reg=0.0, dropout_config=dropout_config)\n",
    "optimizer = AdamOptim(model)\n",
    "hist_dropout_3 = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                           num_epoch=5, batch_size=100, learning_rate=1e-3, learning_decay=0.95,\n",
    "                           verbose=False, record_interval = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small retention rate\n",
    "dropout_config[\"keep_prob\"] = 0.1\n",
    "model = MLP(input_dim=X_train.shape[1], hidden_dims=[200], num_classes=10, \n",
    "            weight_scale=1e-2, l2_reg=0.0, dropout_config=dropout_config)\n",
    "optimizer = AdamOptim(model)\n",
    "hist_dropout_1 = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                           num_epoch=5, batch_size=100, learning_rate=1e-3, learning_decay=0.95,\n",
    "                           verbose=False, record_interval = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_no_dropout, train_acc_no_dropout, val_acc_no_dropout = hist_no_dropout\n",
    "loss_dropout_1, train_acc_dropout_1, val_acc_dropout_1 = hist_dropout_1\n",
    "loss_dropout_3, train_acc_dropout_3, val_acc_dropout_3 = hist_dropout_3\n",
    "loss_dropout_5, train_acc_dropout_5, val_acc_dropout_5 = hist_dropout_5\n",
    "loss_dropout_7, train_acc_dropout_7, val_acc_dropout_7 = hist_dropout_7\n",
    "loss_dropout_9, train_acc_dropout_9, val_acc_dropout_9 = hist_dropout_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_no_dropout, label=\"no dropout\")\n",
    "plt.plot(loss_dropout_1, label=\"keep_prob=0.1\")\n",
    "plt.plot(loss_dropout_3, label=\"keep_prob=0.3\")\n",
    "plt.plot(loss_dropout_5, label=\"keep_prob=0.5\")\n",
    "plt.plot(loss_dropout_7, label=\"keep_prob=0.7\")\n",
    "plt.plot(loss_dropout_9, label=\"keep_prob=0.9\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_acc_no_dropout, label=\"no dropout\")\n",
    "plt.plot(train_acc_dropout_1, label=\"keep_prob=0.1\")\n",
    "plt.plot(train_acc_dropout_3, label=\"keep_prob=0.3\")\n",
    "plt.plot(train_acc_dropout_5, label=\"keep_prob=0.5\")\n",
    "plt.plot(train_acc_dropout_7, label=\"keep_prob=0.7\")\n",
    "plt.plot(train_acc_dropout_9, label=\"keep_prob=0.9\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(val_acc_no_dropout, label=\"no dropout\")\n",
    "plt.plot(val_acc_dropout_1, label=\"keep_prob=0.1\")\n",
    "plt.plot(val_acc_dropout_3, label=\"keep_prob=0.3\")\n",
    "plt.plot(val_acc_dropout_5, label=\"keep_prob=0.5\")\n",
    "plt.plot(val_acc_dropout_7, label=\"keep_prob=0.7\")\n",
    "plt.plot(val_acc_dropout_9, label=\"keep_prob=0.9\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO:</strong></span> Describe what you find in this dropout experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: **[fill in here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Dropout + Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a deep network with both dropout and batch normalization.\n",
    "dropout_config = {\"enabled\": True, \"keep_prob\": 0.6}\n",
    "use_bn = True\n",
    "model = MLP(input_dim=X_train.shape[1], hidden_dims=[100, 50], num_classes=10, \n",
    "            weight_scale=1e-3, l2_reg=0.0, dropout_config=dropout_config, use_bn=use_bn)\n",
    "optimizer = AdamOptim(model)\n",
    "\n",
    "# hist_no_dropout contains loss, train acc and valid acc history.\n",
    "hist_deep_bn = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                           num_epoch=5, batch_size=200, learning_rate=2e-2, learning_decay=0.95, \n",
    "                           verbose=False, record_interval = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "* Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks https://arxiv.org/abs/1602.07868\n",
    "* Highway netowrk https://arxiv.org/pdf/1505.00387.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "36142657f443a869bd2c1b509e6f1df9b014ad48aa206cdd00d27f8f22cb37ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
